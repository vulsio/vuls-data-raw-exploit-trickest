{
  "id": "CVE-2023-29374",
  "tags": [
    {
      "label": "Product",
      "message": "n/a"
    },
    {
      "label": "Version",
      "message": "n/a"
    },
    {
      "label": "Vulnerability",
      "message": "n/a"
    }
  ],
  "description": " In LangChain through 0.0.131, the LLMMathChain chain allows prompt injection attacks that can execute arbitrary code via the Python exec method.",
  "poc": {
    "references": [
      "https://github.com/hwchase17/langchain/issues/1026"
    ],
    "githubs": [
      "https://github.com/cckuailong/awesome-gpt-security",
      "https://github.com/corca-ai/awesome-llm-security"
    ]
  }
}
